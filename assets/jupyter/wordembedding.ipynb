{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5319e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import os  \n",
    "import re\n",
    "from string import digits\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import phrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6919961",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(\"current_dictory\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e391b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dreams.csv')  # Load a CSV file named 'dreams.csv' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c797205c",
   "metadata": {},
   "source": [
    "Open dreams.csv and take a quick look. We can see that the format of dreams is as follows:\n",
    "\n",
    "|   | dreams_text |\n",
    "|------|------------|\n",
    "| 0    | 001 Nightmare in Cambodia. In the dream we are... |\n",
    "| 1    | 002 The enemy is above, in the sky...|\n",
    "| 2    | 003 We are on a firebase. It is night time...|\n",
    "| 3   | 004 We are on an LZ; I am. saying ...|\n",
    "\n",
    "Therefore, we need to extract the text in the second column \"dreams_text\" as our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4799a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the corpus\n",
    "corpus = []\n",
    "# Iterate through each line of text in the CSV file\n",
    "for text in df['dreams_text']:\n",
    "    # Ensure the text is of string type\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3be0ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 30799\n"
     ]
    }
   ],
   "source": [
    "# Check the length of the corpus\n",
    "print(f\"Total documents: {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4257ac76",
   "metadata": {},
   "source": [
    "## Clean the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88fc79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the cleaned corpus\n",
    "corpus_clean = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba1853",
   "metadata": {},
   "source": [
    "Transforms the raw text corpus into a cleaned list of sentences, each composed of words in lowercase, with punctuation and numbers removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fad7023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in corpus:\n",
    "    doc = re.sub(';?:!\"', '.', document)  # Replace semicolons, colons, exclamation marks, and quotation marks with dots, as they will be used to split sentences\n",
    "    doc = re.sub(r'[^\\w\\s.]', '', doc)  # Remove all remaining punctuation marks except for dots\n",
    "    translation_table = str.maketrans('', '', digits)  # Create a translation table to remove digits\n",
    "    doc = doc.translate(translation_table)  # Use the translation table to remove digits from the document\n",
    "    doc = doc.lower()  # Convert all letters in the document to lowercase\n",
    "    doc = re.sub(r'\\s+', ' ', doc)  # Remove any extra spaces\n",
    "    doc = doc.split('.')  # Split the document into sentences using dots as delimiters\n",
    "    doc2 = [j.strip().split(' ') for j in doc]  # Split each sentence into words and remove any remaining extra spaces\n",
    "    for i in doc2:\n",
    "        filter(None, i)  # Remove empty words\n",
    "    corpus_clean.extend(doc2)  # Extend the corpus_clean list with the processed sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06906be1",
   "metadata": {},
   "source": [
    "Thus, we have obtained a list named *corpus_clean*, which is structured as a nested list. Each top-level element in this list is itself a list representing a sentence. Furthermore, each of these sentence lists contains a series of words, with each word being an individual element within the sentence list.\n",
    "\n",
    "| Index   | Value                             |\n",
    "|------|----------------------------------|\n",
    "| 0    | ['nightmare', 'in', 'cambodia']   |\n",
    "| 1    | ['in', 'the', 'dream', 'we', 'are', 'being', 'overrun', 'by', 'sappers', 'who', 'have', 'got', 'past', 'the', 'night', 'defensive', 'perimeter', 'trips', 'and', 'claymores', 'and', 'now', 'crawl', 'forward'] |\n",
    "| 2    | ['i', 'wake', 'up', 'and', 'see', 'a', 'boot', 'tread', 'close', 'to', 'my', 'face'] |\n",
    "| 3    | ['i', 'slowly', 'withdraw', 'my'] |\n",
    "| 4    | ['from', 'its', 'holster', 'pull', 'the', 'hammer', 'back', 'then', 'aim', 'it', 'at', 'the', 'boot'] |\n",
    "| 5    | ['just', 'then', 'the', 'cloudobscured', 'moon', 'comes', 'out', 'and', 'i', 'realize', 'the', 'boot', 'is', 'american', 'and', 'that', 'it', 'is', 'jerry', 'biecks', 'foot'] |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f7fdee",
   "metadata": {},
   "source": [
    "Word embedding inputs are typically structured as lists of words nested within lists of sentences because this format preserves contextual information while offering computational efficiency. It maintains sentence boundaries, facilitating the capture of semantic relationships and enabling easy processing of varying sentence lengths. \n",
    "\n",
    "This structure also supports efficient implementation of sliding window techniques used in many embedding algorithms, retains document structure, and provides flexibility for both sentence-level and corpus-wide operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d3a025",
   "metadata": {},
   "source": [
    "## Next, we can start training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d83234",
   "metadata": {},
   "source": [
    "Before formally training the model, we can use *bigram_transformer* to create a tool for bigram features. A bigram is a statistical language model that considers the joint probability of two adjacent words in a text. This means the model takes into account the order of words, thereby capturing dependencies between them. By using bigrams, we can identify word combinations that frequently occur together, which helps the model better understand the structure and context of language.\n",
    "\n",
    "*phrases.Phrases* is a class, used to identify and create bigrams or trigrams. When you apply this class to a text corpus, it analyzes the corpus and identifies frequently occurring word pairs or word triplets. This process can help us discover common phrases and fixed collocations, thereby improving the model's understanding of language usage patterns. Using this method can significantly enhance the effectiveness of text analysis and natural language processing tasks, especially when dealing with specialized terminology or fixed expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05f94335",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_transformer = phrases.Phrases(corpus_clean) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f6fd165",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_100_10_1HS_samp1 = word2vec.Word2Vec( bigram_transformer[corpus_clean], workers=4, sg=1, hs=1, vector_size=100, window=10, sample=1e-3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6234e822",
   "metadata": {},
   "source": [
    "Explain Parameters：\n",
    "\n",
    "`workers=4`: This parameter specifies the number of threads used during the training process. Set to 4 here, it means that the training process will use 4 threads in parallel to speed up the computation.\n",
    "\n",
    "`sg=1`: This parameter is used to select the type of model for training. sg stands for \"skip-gram,\" and setting it to 1 indicates the use of the skip-gram model. The skip-gram model is suitable for processing smaller corpora and can better capture the relationships of rare words.\n",
    "\n",
    "`hs=1`: This parameter is used to activate the \"hierarchical softmax\" model. Set to 1, it means that hierarchical softmax is enabled. Hierarchical softmax is a technique used to accelerate the training process of word2vec, reducing the computational complexity of the model.\n",
    "\n",
    "`vector_size=100`: This parameter specifies the dimension of the generated word vectors. Set to 100 here, it means that each word will be represented as a 100-dimensional vector.\n",
    "\n",
    "`window=10`: This parameter specifies the context window size for words during training. Set to 10 here, it means that during the training process, the context of each word will include 10 words before and after the target word.\n",
    "\n",
    "`sample=1e-3`: This parameter is used to set the downsampling rate for training data. Set to 1e-3, it means that high-frequency words will be downsampled, appearing less frequently during the training process, reducing their impact on the model and allowing the model to focus more on low-frequency words with greater information content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a072537f",
   "metadata": {},
   "source": [
    "## The model is trained; we can now examine the relationships between some words within this model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d10e99a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'summer'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see if the model can correctly recognize semantics.\n",
    "Model_100_10_1HS_samp1.wv.doesnt_match(\"man woman summer girl\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8a2c3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8032967448234558), ('person', 0.7134841680526733), ('lady', 0.6778110265731812), ('guy', 0.6606528759002686), ('young_man', 0.6053479313850403), ('stranger', 0.6048056483268738), ('lady_who', 0.6013890504837036), ('someone', 0.5913842916488647), ('young_woman', 0.5792549848556519), ('verbal', 0.5633144974708557)]\n"
     ]
    }
   ],
   "source": [
    "print(Model_100_10_1HS_samp1.wv.most_similar(positive=['girl', 'man'], negative=['boy'])) #boy:man as girl:_?___ WOMAN!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100ccd6b",
   "metadata": {},
   "source": [
    "Through some simple examples, it can be seen that our model is capable of identifying semantically different words from \"man woman summer girl\" and can also calculate that \"man - boy + girl ≈ woman\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97d40cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar to  terrifying\n",
      "frightening: 0.733578622341156\n",
      "scarey: 0.6859025359153748\n",
      "nocturnal: 0.6689965128898621\n",
      "nonetheless: 0.6657446026802063\n",
      "very_realistic: 0.6455093622207642\n"
     ]
    }
   ],
   "source": [
    "# demonstrate the five words most similar to the word \"terrifying\" in the Word2Vec model, along with their similarity scores to \"terrifying\".\n",
    "WORD = \"terrifying\"\n",
    "\n",
    "similar_words = Model_100_10_1HS_samp1.wv.most_similar(WORD, topn=5)\n",
    "\n",
    "print(f\"similar to \", WORD)\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6066f069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar to  disturbing\n",
      "nostalgic: 0.5985854268074036\n",
      "embarrassing: 0.5805899500846863\n",
      "ecstatic: 0.5769983530044556\n",
      "deja_vu: 0.5763196349143982\n",
      "creeped_out: 0.571252703666687\n"
     ]
    }
   ],
   "source": [
    "WORD = \"disturbing\"\n",
    "similar_words = Model_100_10_1HS_samp1.wv.most_similar(WORD, topn=5)\n",
    "print(f\"similar to \", WORD)\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "222fcef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar to  sweet\n",
      "tasty: 0.6262686252593994\n",
      "frail: 0.6100146174430847\n",
      "soft: 0.5955066084861755\n",
      "honey: 0.5822334289550781\n",
      "bashful: 0.5809429883956909\n"
     ]
    }
   ],
   "source": [
    "WORD = \"sweet\"\n",
    "similar_words = Model_100_10_1HS_samp1.wv.most_similar(WORD, topn=5)\n",
    "print(f\"similar to \", WORD)\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a732b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('control', 0.4562285244464874), ('outcome', 0.43268337845802307), ('impending', 0.41403695940971375), ('explosion', 0.4125122129917145), ('several_hundred', 0.409423828125), ('fully_awake', 0.406212717294693), ('frightening', 0.39035752415657043), ('crab', 0.38695216178894043), ('source', 0.38543036580085754), ('aware', 0.38370537757873535)]\n"
     ]
    }
   ],
   "source": [
    "#Find words that are semantically similar to the concept represented by the word \"terrifying\" when contrasted with the concept represented by the word \"sweet\"\n",
    "print(Model_100_10_1HS_samp1.wv.most_similar(positive=['terrifying'], negative=['sweet'], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b16b4d1",
   "metadata": {},
   "source": [
    "Exploring word relationships through word embeddings can lead to fascinating insights. These high-dimensional vector representations allow us to:\n",
    "\n",
    "1. Quantify semantic and syntactic properties of words in various dimensions\n",
    "2. Uncover latent patterns and associations in language\n",
    "3. Detect subtle nuances between related concepts\n",
    "4. Analyze how words shift meaning across different contexts\n",
    "5. Understand how complex ideas and emotions are encoded in language\n",
    "\n",
    "Word embeddings enable us to perform arithmetic operations on words, revealing analogies and conceptual relationships. For instance, the example \"man - boy + girl ≈ woman\" demonstrates how these models capture gender relationships. Such capabilities not only enhance our understanding of linguistic structures but also power numerous natural language processing applications, from machine translation to sentiment analysis.\n",
    "\n",
    "Furthermore, by examining the cosine similarity between word vectors, we can identify synonyms, antonyms, and words with similar usage patterns. This allows for a more nuanced exploration of language, going beyond simple dictionary definitions to understand the multifaceted nature of word meanings and their interconnections within the broader lexical landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5095c7eb",
   "metadata": {},
   "source": [
    "Next, we can measure how closely a piece of text semantically aligns with a specific feature, such as 'terrifying,' by comparing the cosine similarity between the text and that feature. This approach allows us to estimate the degree of terror conveyed in a dream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bba7995a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of the text is: 0.42425134778022766\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np  # Import numpy\n",
    "\n",
    "def measure_dreams(model, text, feature):\n",
    "    # Preprocess the text\n",
    "    words = simple_preprocess(text)\n",
    "    \n",
    "    # Get the vector for the feature you want to calculate\n",
    "    feature_vector = model.wv[feature]  # Use the feature variable\n",
    "    \n",
    "    # Calculate the average vector for the text\n",
    "    text_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if not text_vectors:\n",
    "        return 0  # Return 0 if no words are in the vocabulary\n",
    "    text_vector = np.mean(text_vectors, axis=0)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = np.dot(feature_vector, text_vector) / (np.linalg.norm(feature_vector) * np.linalg.norm(text_vector))\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "# Example usage\n",
    "text = \" Nightmare in Cambodia. In the dream we are being overrun by sappers who have got past the Night Defensive Perimeter trips and claymores and now crawl forward. I wake up and see a boot tread close to my face. I slowly withdraw my .45 from its holster, pull the hammer back, then aim it at the boot. Just then the cloud-obscured moon comes out and I realize the boot is American and that it is Jerry Bieck's foot. In the pitch stillness I point the .45 straight up in the air. Pinching the hammer tightly I pull the trigger and settle the hammer back in place. I re-holster the pistol and go back to sleep. The next day, after a very difficult march, all the men are overjoyed to be out of Cambodia. I tell no one what almost happened.\"\n",
    "feature_score = measure_dreams(Model_100_10_1HS_samp1, text, 'terrifying')  # Use quotes for the feature\n",
    "print(f\"The score of the text is: {feature_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db050eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of the text is: 0.3600655198097229\n"
     ]
    }
   ],
   "source": [
    "text = \"I know my ex-boyfriend from college, Tracey was in the dream, although not sure I remember much about that. I was at the mall walking with someone and I ran into these two people. One of them was Teri this girl I went to college with. We were really good friends in college. I was like 'TERI!!' and we hugged, it was very nice to see her. I remember telling her that I had been to Virginia beach a few times already and would be coming back this year. She is from va beach There was much more to this dream, just cannot remember.\"\n",
    "feature_score = measure_dreams(Model_100_10_1HS_samp1, text, 'terrifying')  # Use quotes for the feature\n",
    "print(f\"The score of the text is: {feature_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
