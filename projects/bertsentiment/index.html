<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Sentiment Classification Using Fine-tuned BERT | Haotian Zhao </title> <meta name="author" content="Haotian Zhao"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%A1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://haotianzhao99.github.io/projects/bertsentiment/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Haotian</span> Zhao </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/academicpapers/">academic papers </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Sentiment Classification Using Fine-tuned BERT</h1> <p class="post-description"></p> </header> <article> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">TrainingArguments</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_recall_fscore_support</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
</code></pre></div></div> <p>Before diving into the implementation, we need to set up our authentication with the <a href="https://huggingface.co/" rel="external nofollow noopener" target="_blank">Hugging Face Hub</a>. <a href="https://huggingface.co/" rel="external nofollow noopener" target="_blank">Hugging Face</a> is a platform that hosts thousands of pre-trained models and datasets, making it an essential resource for modern NLP tasks. This step is crucial if you plan to work with private models or want to save your fine-tuned model to the Hub later.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>

<span class="c1"># Log in to Hugging Face Hub using authentication token
# Required for accessing private models and pushing models to Hub
</span><span class="nf">login</span><span class="p">(</span><span class="sh">"</span><span class="s">your_token</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="data-loading-and-preparation">Data Loading and Preparation</h2> <p>For this sentiment analysis task, we’ll use a Chinese social media dataset containing 100,000 Weibo posts with sentiment labels. The dataset is hosted on the <a href="https://huggingface.co/datasets/dirtycomputer/weibo_senti_100k" rel="external nofollow noopener" target="_blank">Hugging Face Hub</a> and can be easily loaded using the <code class="language-plaintext highlighter-rouge">datasets</code> library.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Load sentiment analysis dataset from Hugging Face Hub
# Dataset contains 100k Weibo posts with sentiment labels
</span><span class="n">ds</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">dirtycomputer/weibo_senti_100k</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ds</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DatasetDict({
    train: Dataset({
        features: ['label', 'review'],
        num_rows: 119988
    })
})
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Convert Hugging Face dataset to pandas DataFrame
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">])</span>

<span class="c1"># Display basic information about the DataFrame
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Dataset Overview:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Number of samples: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Columns: </span><span class="si">{</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="nf">tolist</span><span class="p">()</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Label distribution:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Sample reviews:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">review</span><span class="sh">'</span><span class="p">].</span><span class="nf">head</span><span class="p">())</span>

<span class="c1"># Check for any missing values
</span><span class="k">if</span> <span class="n">df</span><span class="p">.</span><span class="nf">isnull</span><span class="p">().</span><span class="nf">sum</span><span class="p">().</span><span class="nf">any</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Warning: Dataset contains missing values!</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Dataset Overview:
Number of samples: 119988
Columns: ['label', 'review']

Label distribution:
label
0    59995
1    59993
Name: count, dtype: int64

Sample reviews:
0                ﻿更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]
1    @张晓鹏jonathan 土耳其的事要认真对待[哈哈]，否则直接开除。@丁丁看世界 很是细心...
2    姑娘都羡慕你呢…还有招财猫高兴……//@爱在蔓延-JC:[哈哈]小学徒一枚，等着明天见您呢/...
3                                           美~~~~~[爱你]
4                                    梦想有多大，舞台就有多大![鼓掌]
Name: review, dtype: object
</code></pre></div></div> <p>The dataset contains 119,988 samples. The dataset is perfectly balanced with 59,995 negative samples (label 0) and 59,993 positive samples (label 1).</p> <h2 id="data-splitting">Data Splitting</h2> <p>Although our dataset is pre-organized, we’ll create our own train-test split to ensure we have a fresh evaluation set. We’ll use 80% of the data for training and reserve 20% for testing the model’s performance.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Split dataset into training and test sets
# - test_size=0.2: 80% training, 20% testing
# - shuffle=True: randomly shuffle before splitting
# - random_state=42: set seed for reproducibility
</span><span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div></div> <p>The key parameters:</p> <ol> <li> <code class="language-plaintext highlighter-rouge">test_size=0.2</code>: Creates an 80-20 split, with ~96,000 training samples and ~24,000 test samples</li> <li> <code class="language-plaintext highlighter-rouge">shuffle=True</code>: Ensures random distribution of data, preventing ordering bias</li> <li> <code class="language-plaintext highlighter-rouge">random_state=42</code>: Sets a seed for reproducible results</li> </ol> <p>I run the project on <a href="https://colab.research.google.com" rel="external nofollow noopener" target="_blank">Google Colab</a>, a cloud-based Jupyter notebook environment. Colab provides free GPU access, making it an excellent choice for users without local GPU resources to run deep learning models like BERT.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Check for available CUDA device and set up GPU/CPU
# Colab typically provides a single GPU, if available
</span><span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">():</span>
   <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">)</span>
   <span class="c1"># Print GPU information
</span>   <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Using GPU: </span><span class="si">{</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">get_device_name</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
   <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">GPU Memory: </span><span class="si">{</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">get_device_properties</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">total_memory</span> <span class="o">/</span> <span class="mf">1e9</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> GB</span><span class="sh">"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
   <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
   <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">No GPU available, using CPU</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Using GPU: NVIDIA A100-SXM4-40GB
GPU Memory: 42.48 GB
</code></pre></div></div> <h2 id="tokenizer-initialization">Tokenizer Initialization</h2> <p>For our Chinese sentiment analysis task, we’ll use the <code class="language-plaintext highlighter-rouge">bert-base-chinese</code> tokenizer. This pre-trained tokenizer is specifically designed for Chinese text.</p> <p>The tokenizer is crucial for preparing our text data for BERT. It converts Chinese text into tokens that BERT can understand</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initialize the BERT Chinese tokenizer
# Uses bert-base-chinese pre-trained model's vocabulary
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">bert-base-chinese</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>To efficiently handle our data during training, we need to create a custom Dataset class that inherits from PyTorch’s Dataset class. This class will take care of text encoding and provide a standardized way to access our samples.</p> <p>It serves as a data pipeline that:</p> <ol> <li>Transforms Chinese text into BERT-compatible token IDs</li> <li>Ensures consistent input dimensions through padding and truncation</li> <li>Efficiently delivers batched data during training</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="k">class</span> <span class="nc">TextDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
   <span class="sh">"""</span><span class="s">
   Custom Dataset class for text data, inheriting from PyTorch</span><span class="sh">'</span><span class="s">s Dataset.

   Parameters:
   tokenizer (Tokenizer): Tokenizer object for text encoding
   texts (list): List of text samples
   labels (list): List of corresponding labels
   </span><span class="sh">"""</span>
   <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">texts</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
       <span class="c1"># Encode texts with padding and truncation
</span>       <span class="n">self</span><span class="p">.</span><span class="n">encodings</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
           <span class="n">texts</span><span class="p">,</span>
           <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
           <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
           <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>  <span class="c1"># Explicitly set max length for BERT
</span>           <span class="n">return_tensors</span><span class="o">=</span><span class="sh">'</span><span class="s">pt</span><span class="sh">'</span>  <span class="c1"># Return PyTorch tensors directly
</span>       <span class="p">)</span>
       <span class="c1"># Convert labels to tensor
</span>       <span class="n">self</span><span class="p">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

   <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
       <span class="sh">"""</span><span class="s">
       Get a single sample by index.

       Args:
           idx (int): Sample index

       Returns:
           dict: Dictionary containing encoded text data and label
       </span><span class="sh">"""</span>
       <span class="k">return</span> <span class="p">{</span>
           <span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">encodings</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">][</span><span class="n">idx</span><span class="p">],</span>
           <span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">encodings</span><span class="p">[</span><span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">][</span><span class="n">idx</span><span class="p">],</span>
           <span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
       <span class="p">}</span>

   <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
       <span class="sh">"""</span><span class="s">
       Get dataset length.

       Returns:
           int: Number of samples in dataset
       </span><span class="sh">"""</span>
       <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">labels</span><span class="p">)</span>
</code></pre></div></div> <p>Let’s verify our label distribution and create an explicit mapping for our sentiment classes. While our labels are already in a binary format (0 and 1), maintaining an explicit mapping is a good practice for code clarity and future modifications.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Print unique labels in the dataset
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Unique labels in training data:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">sorted</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">])))</span>

<span class="c1"># Create explicit label mapping
</span><span class="n">label_mapping</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mi">0</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>  <span class="c1"># Negative
</span>    <span class="mi">1</span><span class="p">:</span> <span class="mi">1</span>   <span class="c1"># Positive
</span><span class="p">}</span>

<span class="c1"># Map labels using explicit mapping
</span><span class="n">train_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">label_mapping</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]]</span>
<span class="n">test_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">label_mapping</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">test_df</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]]</span>

<span class="c1"># Verify label distribution after mapping
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Label distribution after mapping:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Training:</span><span class="sh">"</span><span class="p">,</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">train_labels</span><span class="p">).</span><span class="nf">value_counts</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Testing:</span><span class="sh">"</span><span class="p">,</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">test_labels</span><span class="p">).</span><span class="nf">value_counts</span><span class="p">())</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Unique labels in training data: [0, 1]

Label distribution after mapping:
Training: 0    48151
1    47839
Name: count, dtype: int64
Testing: 1    12154
0    11844
Name: count, dtype: int64
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Explicit label mapping to ensure correct sentiment assignment
</span><span class="n">label_to_index</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mi">0</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>  <span class="c1"># Keep negative as 0
</span>    <span class="mi">1</span><span class="p">:</span> <span class="mi">1</span>   <span class="c1"># Keep positive as 1
</span><span class="p">}</span>

<span class="c1"># Map labels using explicit mapping
</span><span class="n">train_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">label_to_index</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]]</span>
<span class="n">test_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">label_to_index</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">test_df</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]]</span>

<span class="c1"># Create datasets with verified labels
</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="nc">TextDataset</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">review</span><span class="sh">'</span><span class="p">].</span><span class="nf">tolist</span><span class="p">(),</span> <span class="n">train_labels</span><span class="p">)</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="nc">TextDataset</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">test_df</span><span class="p">[</span><span class="sh">'</span><span class="s">review</span><span class="sh">'</span><span class="p">].</span><span class="nf">tolist</span><span class="p">(),</span> <span class="n">test_labels</span><span class="p">)</span>

<span class="c1"># Verify final mapping
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Final verification:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Training set label distribution:</span><span class="sh">"</span><span class="p">,</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">train_labels</span><span class="p">).</span><span class="nf">value_counts</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Test set label distribution:</span><span class="sh">"</span><span class="p">,</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">test_labels</span><span class="p">).</span><span class="nf">value_counts</span><span class="p">())</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Final verification:
Training set label distribution: 0    48151
1    47839
Name: count, dtype: int64
Test set label distribution: 1    12154
0    11844
Name: count, dtype: int64
</code></pre></div></div> <h2 id="model-training-setup">Model Training Setup</h2> <p>To fine-tune BERT for our sentiment analysis task, we’ll follow these key steps:</p> <ol> <li> <strong>Model Initialization</strong>: Load the pre-trained Chinese BERT model</li> <li> <strong>Training Configuration</strong>: Set up training parameters using <code class="language-plaintext highlighter-rouge">TrainingArguments</code> </li> <li> <strong>Metrics Setup</strong>: Define evaluation metrics for model performance monitoring</li> <li> <strong>Trainer Setup</strong>: Initialize the Hugging Face <code class="language-plaintext highlighter-rouge">Trainer</code> class with: <ul> <li>The BERT model</li> <li>Training arguments</li> <li>Training and evaluation datasets</li> <li>Metrics computation function</li> </ul> </li> <li> <strong>Training Process</strong>: Use <code class="language-plaintext highlighter-rouge">trainer.train()</code> and <code class="language-plaintext highlighter-rouge">trainer.evaluate()</code> for model fine-tuning and evaluation</li> </ol> <p>The Hugging Face Trainer API simplifies the training process by handling the training loops, device management, and model optimization automatically.</p> <p>The code below implements these steps:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load pre-trained Chinese BERT model and configure for binary classification
</span><span class="n">model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
   <span class="sh">'</span><span class="s">bert-base-chinese</span><span class="sh">'</span><span class="p">,</span>
   <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span>  <span class="c1"># Binary classification (negative/positive)
</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Define training arguments for model fine-tuning
</span><span class="n">training_args</span> <span class="o">=</span> <span class="nc">TrainingArguments</span><span class="p">(</span>
   <span class="n">output_dir</span><span class="o">=</span><span class="sh">'</span><span class="s">sentiment-weibo-100k-fine-tuned-bert-test</span><span class="sh">'</span><span class="p">,</span>  <span class="c1"># Directory to save model checkpoints
</span>   <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>                                 <span class="c1"># Number of training epochs
</span>   <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>                    <span class="c1"># Number of samples per training batch
</span>   <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>                     <span class="c1"># Number of samples per evaluation batch
</span>   <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>                                  <span class="c1"># Steps for learning rate warmup
</span>   <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>                                 <span class="c1"># L2 regularization factor
</span>   <span class="n">logging_dir</span><span class="o">=</span><span class="sh">'</span><span class="s">./logs</span><span class="sh">'</span><span class="p">,</span>                             <span class="c1"># Directory for training logs
</span>   <span class="n">logging_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>                                <span class="c1"># Log metrics every 100 steps
</span>   <span class="n">evaluation_strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">,</span>                      <span class="c1"># Evaluate after each epoch
</span>   <span class="n">save_strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">,</span>                           <span class="c1"># Save model after each epoch
</span>   <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>                    <span class="c1"># Load best model after training
</span>   <span class="n">push_to_hub</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>                               <span class="c1"># Push model to Hugging Face Hub
</span>   <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>                             <span class="c1"># Initial learning rate
</span>   <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">1</span>                   <span class="c1"># Update model after every batch
</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">pred</span><span class="p">):</span>
   <span class="sh">"""</span><span class="s">
   Compute evaluation metrics for the model
   Args:
       pred: Contains predictions and label_ids
   Returns:
       dict: Dictionary containing accuracy, F1, precision, and recall scores
   </span><span class="sh">"""</span>
   <span class="n">labels</span> <span class="o">=</span> <span class="n">pred</span><span class="p">.</span><span class="n">label_ids</span>
   <span class="n">preds</span> <span class="o">=</span> <span class="n">pred</span><span class="p">.</span><span class="n">predictions</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
   <span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">f1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">precision_recall_fscore_support</span><span class="p">(</span>
       <span class="n">labels</span><span class="p">,</span>
       <span class="n">preds</span><span class="p">,</span>
       <span class="n">average</span><span class="o">=</span><span class="sh">'</span><span class="s">binary</span><span class="sh">'</span><span class="p">,</span>
       <span class="n">pos_label</span><span class="o">=</span><span class="mi">1</span>  <span class="c1"># Define positive class for binary metrics
</span>   <span class="p">)</span>
   <span class="n">acc</span> <span class="o">=</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>

   <span class="k">return</span> <span class="p">{</span>
       <span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>
       <span class="sh">'</span><span class="s">f1</span><span class="sh">'</span><span class="p">:</span> <span class="n">f1</span><span class="p">,</span>
       <span class="sh">'</span><span class="s">precision</span><span class="sh">'</span><span class="p">:</span> <span class="n">precision</span><span class="p">,</span>
       <span class="sh">'</span><span class="s">recall</span><span class="sh">'</span><span class="p">:</span> <span class="n">recall</span>
   <span class="p">}</span>

<span class="c1"># Initialize trainer with model and training configuration
</span><span class="n">trainer</span> <span class="o">=</span> <span class="nc">Trainer</span><span class="p">(</span>
   <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
   <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
   <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
   <span class="n">eval_dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span>
   <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span>
<span class="p">)</span>

<span class="c1"># Print training configuration summary
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Training Configuration:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Model: bert-base-chinese</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Training samples: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Test samples: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Batch size: </span><span class="si">{</span><span class="n">training_args</span><span class="p">.</span><span class="n">per_device_train_batch_size</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Number of epochs: </span><span class="si">{</span><span class="n">training_args</span><span class="p">.</span><span class="n">num_train_epochs</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Start training
</span><span class="n">trainer</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>

<span class="c1"># Evaluate model performance
</span><span class="n">eval_results</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Evaluation Results:</span><span class="sh">"</span><span class="p">,</span> <span class="n">eval_results</span><span class="p">)</span>
</code></pre></div></div> <div class="row justify-content-center"> <div class="col-sm-10 col-md-8 col-lg-9"> <figure class="figure"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project/sentiment/sentiment1.png" sizes="95vw"></source> <img src="/assets/img/project/sentiment/sentiment1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="figure-caption text-center"> </figcaption> </figure> </div> </div> <p>After completing the model training, let’s test it out.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="k">def</span> <span class="nf">test_sentiment</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">yourmodel</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Test sentiment analysis model with given texts
    </span><span class="sh">"""</span>
    <span class="c1"># Create sentiment analyzer pipeline
</span>    <span class="n">analyzer</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">sentiment-analysis</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="n">yourmodel</span><span class="p">,</span>  <span class="c1"># your model from HuggingFace Hub
</span>        <span class="n">tokenizer</span><span class="o">=</span><span class="sh">"</span><span class="s">bert-base-chinese</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="mi">0</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>
    <span class="p">)</span>

    <span class="c1"># Process each text
</span>    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="nf">analyzer</span><span class="p">(</span><span class="n">text</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">sentiment</span> <span class="o">=</span> <span class="sh">"</span><span class="s">positive</span><span class="sh">"</span> <span class="k">if</span> <span class="n">result</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="sh">'</span><span class="s">LABEL_1</span><span class="sh">'</span> <span class="k">else</span> <span class="sh">"</span><span class="s">negative</span><span class="sh">"</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Text: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Sentiment: </span><span class="si">{</span><span class="n">sentiment</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Confidence: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Test with example texts
</span><span class="n">test_texts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">这家店的菜真香，下次还来！</span><span class="sh">"</span><span class="p">,</span>         <span class="c1"># The food is delicious, will come again
</span>    <span class="sh">"</span><span class="s">质量有问题，不推荐购买。</span><span class="sh">"</span><span class="p">,</span>           <span class="c1"># Quality issues, not recommended
</span>    <span class="sh">"</span><span class="s">快递很快，包装完整。</span><span class="sh">"</span><span class="p">,</span>               <span class="c1"># Fast delivery, good packaging
</span>    <span class="sh">"</span><span class="s">商家态度很不好，生气。</span><span class="sh">"</span><span class="p">,</span>             <span class="c1"># Bad merchant attitude, angry
</span>    <span class="sh">"</span><span class="s">非常满意，超出预期。</span><span class="sh">"</span><span class="p">,</span>               <span class="c1"># Very satisfied, exceeded expectations
</span>    <span class="sh">"</span><span class="s">难吃到极点，太糟糕了。</span><span class="sh">"</span><span class="p">,</span>             <span class="c1"># Extremely bad taste, terrible
</span>    <span class="sh">"</span><span class="s">穿着很舒服，尺码合适。</span><span class="sh">"</span><span class="p">,</span>             <span class="c1"># Comfortable to wear, good size
</span>    <span class="sh">"</span><span class="s">卖家服务特别好！</span><span class="sh">"</span><span class="p">,</span>                   <span class="c1"># Great service from seller
</span>    <span class="sh">"</span><span class="s">不值这个价钱，后悔买了。</span><span class="sh">"</span><span class="p">,</span>           <span class="c1"># Not worth the price, regret buying
</span>    <span class="sh">"</span><span class="s">产品完全是垃圾，气死了。</span><span class="sh">"</span>            <span class="c1"># Product is totally garbage, very angry
</span><span class="p">]</span>

<span class="c1"># Run the test
</span><span class="nf">test_sentiment</span><span class="p">(</span><span class="n">test_texts</span><span class="p">,</span> <span class="sh">"</span><span class="s">BarryzZ/sentiment-weibo-100k-fine-tuned-bert-test</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Device set to use cuda:0



Text: 这家店的菜真香，下次还来！
Sentiment: positive
Confidence: 1.0000

Text: 质量有问题，不推荐购买。
Sentiment: positive
Confidence: 1.0000

Text: 快递很快，包装完整。
Sentiment: positive
Confidence: 1.0000

Text: 商家态度很不好，生气。
Sentiment: positive
Confidence: 1.0000

Text: 非常满意，超出预期。
Sentiment: positive
Confidence: 1.0000

Text: 难吃到极点，太糟糕了。
Sentiment: positive
Confidence: 1.0000

Text: 穿着很舒服，尺码合适。
Sentiment: positive
Confidence: 1.0000

Text: 卖家服务特别好！
Sentiment: positive
Confidence: 1.0000

Text: 不值这个价钱，后悔买了。
Sentiment: positive
Confidence: 1.0000

Text: 产品完全是垃圾，气死了。
Sentiment: positive
Confidence: 0.9997
</code></pre></div></div> <p>There’s clearly an issue with our model’s predictions. The model is:</p> <ol> <li>Classifying everything as positive (positive sentiment)</li> <li>Doing so with extremely high confidence (nearly 100%)</li> <li>Failing to identify obvious negative sentiments like “难吃到极点” and “产品完全是垃圾”</li> </ol> <p>These issues are likely due to optimization problems rather than data imbalance. Our adjustments focus on:</p> <ol> <li>Better monitoring (more frequent evaluation, detailed metrics)</li> <li>Improved efficiency (larger batches, mixed precision)</li> <li>Extended training (more epochs, early stopping)</li> </ol> <p>Let’s test the model with these optimized parameters.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initialize model with same configuration
</span><span class="n">model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
   <span class="sh">'</span><span class="s">bert-base-chinese</span><span class="sh">'</span><span class="p">,</span>
   <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Enhanced training arguments
</span><span class="n">training_args</span> <span class="o">=</span> <span class="nc">TrainingArguments</span><span class="p">(</span>
   <span class="n">output_dir</span><span class="o">=</span><span class="sh">'</span><span class="s">sentiment-weibo-100k-fine-tuned-bert</span><span class="sh">'</span><span class="p">,</span>
   <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>                    <span class="c1"># Increased from 3 to 5 for better learning
</span>   <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>        <span class="c1"># Doubled for faster training
</span>   <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>        <span class="c1"># Doubled for faster evaluation
</span>   <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>                    <span class="c1"># Kept same learning rate
</span>   <span class="n">warmup_ratio</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>                      <span class="c1"># Added warmup ratio for smoother training
</span>   <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>                     <span class="c1"># For regularization
</span>   <span class="n">logging_dir</span><span class="o">=</span><span class="sh">'</span><span class="s">./logs</span><span class="sh">'</span><span class="p">,</span>
   <span class="n">logging_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
   <span class="n">evaluation_strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">steps</span><span class="sh">"</span><span class="p">,</span>           <span class="c1"># Changed to step-based evaluation
</span>   <span class="n">eval_steps</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>                        <span class="c1"># More frequent evaluation
</span>   <span class="n">save_strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">steps</span><span class="sh">"</span><span class="p">,</span>
   <span class="n">save_steps</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>                        <span class="c1"># More frequent model saving
</span>   <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
   <span class="n">metric_for_best_model</span><span class="o">=</span><span class="sh">"</span><span class="s">f1_avg</span><span class="sh">"</span><span class="p">,</span>        <span class="c1"># Using average F1 score to select best model
</span>   <span class="n">push_to_hub</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
   <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
   <span class="n">fp16</span><span class="o">=</span><span class="bp">True</span>                              <span class="c1"># Added mixed precision training for efficiency
</span><span class="p">)</span>

<span class="c1"># Enhanced metrics computation function
</span><span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">pred</span><span class="p">):</span>
   <span class="sh">"""</span><span class="s">
   Compute detailed metrics including class-specific scores
   Returns metrics for both positive and negative classes
   </span><span class="sh">"""</span>
   <span class="n">labels</span> <span class="o">=</span> <span class="n">pred</span><span class="p">.</span><span class="n">label_ids</span>
   <span class="n">preds</span> <span class="o">=</span> <span class="n">pred</span><span class="p">.</span><span class="n">predictions</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

   <span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">f1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">precision_recall_fscore_support</span><span class="p">(</span>
       <span class="n">labels</span><span class="p">,</span>
       <span class="n">preds</span><span class="p">,</span>
       <span class="n">average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
       <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
   <span class="p">)</span>
   <span class="n">acc</span> <span class="o">=</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>
   <span class="n">conf_mat</span> <span class="o">=</span> <span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>

   <span class="k">return</span> <span class="p">{</span>
       <span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span>
       <span class="sh">'</span><span class="s">f1_neg</span><span class="sh">'</span><span class="p">:</span> <span class="n">f1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>                   <span class="c1"># Added separate F1 scores
</span>       <span class="sh">'</span><span class="s">f1_pos</span><span class="sh">'</span><span class="p">:</span> <span class="n">f1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
       <span class="sh">'</span><span class="s">f1_avg</span><span class="sh">'</span><span class="p">:</span> <span class="n">f1</span><span class="p">.</span><span class="nf">mean</span><span class="p">(),</span>
       <span class="sh">'</span><span class="s">precision_neg</span><span class="sh">'</span><span class="p">:</span> <span class="n">precision</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>      <span class="c1"># Added class-specific precision
</span>       <span class="sh">'</span><span class="s">precision_pos</span><span class="sh">'</span><span class="p">:</span> <span class="n">precision</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
       <span class="sh">'</span><span class="s">recall_neg</span><span class="sh">'</span><span class="p">:</span> <span class="n">recall</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>           <span class="c1"># Added class-specific recall
</span>       <span class="sh">'</span><span class="s">recall_pos</span><span class="sh">'</span><span class="p">:</span> <span class="n">recall</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
       <span class="sh">'</span><span class="s">confusion_matrix</span><span class="sh">'</span><span class="p">:</span> <span class="n">conf_mat</span><span class="p">.</span><span class="nf">tolist</span><span class="p">()</span>  <span class="c1"># Added confusion matrix
</span>   <span class="p">}</span>


<span class="c1"># Initialize trainer
</span><span class="n">trainer</span> <span class="o">=</span> <span class="nc">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="nc">EarlyStoppingCallback</span><span class="p">(</span><span class="n">early_stopping_patience</span><span class="o">=</span><span class="mi">3</span><span class="p">)]</span>
<span class="p">)</span>

<span class="c1"># Print dataset statistics before training
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Dataset Statistics:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Training samples: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Test samples: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Label Distribution:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Training:</span><span class="sh">"</span><span class="p">,</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">([</span><span class="n">d</span><span class="p">[</span><span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">train_dataset</span><span class="p">]).</span><span class="nf">value_counts</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Testing:</span><span class="sh">"</span><span class="p">,</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">([</span><span class="n">d</span><span class="p">[</span><span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">test_dataset</span><span class="p">]).</span><span class="nf">value_counts</span><span class="p">())</span>

<span class="c1"># Start training
</span><span class="n">trainer</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>

<span class="c1"># Evaluate model
</span><span class="n">eval_results</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Final Evaluation Results:</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">metric</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">eval_results</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">value</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="row justify-content-center"> <div class="col-sm-12 col-md-11 col-lg-10"> <figure class="figure"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project/sentiment/sentiment2.png" sizes="95vw"></source> <img src="/assets/img/project/sentiment/sentiment2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="figure-caption text-center"> </figcaption> </figure> </div> </div> <p>The model achieved excellent metrics after just one epoch.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Test with example texts
</span><span class="n">test_texts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">这家店的菜真香，下次还来！</span><span class="sh">"</span><span class="p">,</span>         <span class="c1"># The food is delicious, will come again
</span>    <span class="sh">"</span><span class="s">质量有问题，不推荐。</span><span class="sh">"</span><span class="p">,</span>           <span class="c1"># Quality issues, not recommended
</span>    <span class="sh">"</span><span class="s">快递很快，包装完整。</span><span class="sh">"</span><span class="p">,</span>               <span class="c1"># Fast delivery, good packaging
</span>    <span class="sh">"</span><span class="s">商家态度不好，生气。</span><span class="sh">"</span><span class="p">,</span>             <span class="c1"># Bad merchant attitude, angry
</span>    <span class="sh">"</span><span class="s">非常满意，超出预期。</span><span class="sh">"</span><span class="p">,</span>               <span class="c1"># Very satisfied, exceeded expectations
</span>    <span class="sh">"</span><span class="s">难吃到极点，太糟糕了。</span><span class="sh">"</span><span class="p">,</span>             <span class="c1"># Extremely bad taste, terrible
</span>    <span class="sh">"</span><span class="s">穿着很舒服，尺码合适。</span><span class="sh">"</span><span class="p">,</span>             <span class="c1"># Comfortable to wear, good size
</span>    <span class="sh">"</span><span class="s">卖家服务特别好！</span><span class="sh">"</span><span class="p">,</span>                   <span class="c1"># Great service from seller
</span>    <span class="sh">"</span><span class="s">不值这个价钱，后悔买了。</span><span class="sh">"</span><span class="p">,</span>           <span class="c1"># Not worth the price, regret buying
</span>    <span class="sh">"</span><span class="s">产品完全是垃圾，气死了。</span><span class="sh">"</span>            <span class="c1"># Product is totally garbage, very angry
</span><span class="p">]</span>

<span class="c1"># Run the test
</span><span class="nf">test_sentiment</span><span class="p">(</span><span class="n">test_texts</span><span class="p">,</span> <span class="sh">"</span><span class="s">BarryzZ/sentiment-weibo-100k-fine-tuned-bert</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Device set to use cuda:0



Text: 这家店的菜真香，下次还来！
Sentiment: positive
Confidence: 0.9923

Text: 质量有问题，不推荐。
Sentiment: negative
Confidence: 0.8533

Text: 快递很快，包装完整。
Sentiment: positive
Confidence: 0.9878

Text: 商家态度不好，生气。
Sentiment: negative
Confidence: 0.9732

Text: 非常满意，超出预期。
Sentiment: positive
Confidence: 0.9791

Text: 难吃到极点，太糟糕了。
Sentiment: negative
Confidence: 0.8653

Text: 穿着很舒服，尺码合适。
Sentiment: positive
Confidence: 0.9907

Text: 卖家服务特别好！
Sentiment: positive
Confidence: 0.9922

Text: 不值这个价钱，后悔买了。
Sentiment: negative
Confidence: 0.8147

Text: 产品完全是垃圾，气死了。
Sentiment: negative
Confidence: 0.9863
</code></pre></div></div> <p>After parameter optimization, our model shows significant improvements.</p> <p>Let’s test it with some new scenarios to verify its robustness.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_texts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="c1"># Strong positive / 强烈正面
</span>    <span class="sh">"</span><span class="s">我考上研究生了！</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># I got accepted into graduate school!
</span>    <span class="sh">"</span><span class="s">今天他向我求婚了！</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># He proposed to me today!
</span>    <span class="sh">"</span><span class="s">终于买到梦想的房子</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Finally bought my dream house
</span>    <span class="sh">"</span><span class="s">中了五百万大奖！</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Won a 5 million prize!
</span>
    <span class="c1"># Strong negative / 强烈负面
</span>    <span class="sh">"</span><span class="s">被裁员了，好绝望</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Got laid off, feeling desperate
</span>    <span class="sh">"</span><span class="s">信任的人背叛我</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Betrayed by someone I trusted
</span>    <span class="sh">"</span><span class="s">重要的文件全丢了</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Lost all important documents
</span>    <span class="sh">"</span><span class="s">又被扣工资了，气死</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Got my salary deducted again, so angry
</span>
    <span class="c1"># Anger / 愤怒
</span>    <span class="sh">"</span><span class="s">偷我的车，混蛋！</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Someone stole my car, bastard!
</span>    <span class="sh">"</span><span class="s">骗子公司，我要报警</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Scam company, I'm calling the police
</span>    <span class="sh">"</span><span class="s">半夜装修，烦死了</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Renovation at midnight, so annoying
</span>    <span class="sh">"</span><span class="s">商家太坑人了！</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># The merchant is such a ripoff!
</span>
    <span class="c1"># Pleasant surprise / 惊喜
</span>    <span class="sh">"</span><span class="s">宝宝会走路了！</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Baby learned to walk!
</span>    <span class="sh">"</span><span class="s">升职加薪啦！</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Got promoted with a raise!
</span>    <span class="sh">"</span><span class="s">论文发表成功！</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Paper got published successfully!
</span>    <span class="sh">"</span><span class="s">收到offer了！</span><span class="sh">"</span>  <span class="c1"># Received a job offer!
</span><span class="p">]</span>
<span class="c1"># Run the test
</span><span class="nf">test_sentiment</span><span class="p">(</span><span class="n">test_texts</span><span class="p">,</span> <span class="sh">"</span><span class="s">BarryzZ/sentiment-weibo-100k-fine-tuned-bert</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Device set to use cuda:0
You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset



Text: 我考上研究生了！
Sentiment: positive
Confidence: 0.8713

Text: 今天他向我求婚了！
Sentiment: positive
Confidence: 0.6087

Text: 终于买到梦想的房子
Sentiment: positive
Confidence: 0.7931

Text: 中了五百万大奖！
Sentiment: positive
Confidence: 0.6070

Text: 被裁员了，好绝望
Sentiment: negative
Confidence: 0.9973

Text: 信任的人背叛我
Sentiment: negative
Confidence: 0.9572

Text: 重要的文件全丢了
Sentiment: negative
Confidence: 0.9941

Text: 又被扣工资了，气死
Sentiment: negative
Confidence: 0.9963

Text: 偷我的车，混蛋！
Sentiment: negative
Confidence: 0.9664

Text: 骗子公司，我要报警
Sentiment: negative
Confidence: 0.9750

Text: 半夜装修，烦死了
Sentiment: negative
Confidence: 0.9906

Text: 商家太坑人了！
Sentiment: negative
Confidence: 0.8367

Text: 宝宝会走路了！
Sentiment: positive
Confidence: 0.9125

Text: 升职加薪啦！
Sentiment: positive
Confidence: 0.9727

Text: 论文发表成功！
Sentiment: positive
Confidence: 0.9998

Text: 收到offer了！
Sentiment: positive
Confidence: 0.7036
</code></pre></div></div> <p>The optimized model shows excellent performance in Chinese sentiment analysis. It now correctly identifies both positive and negative sentiments with appropriate confidence levels, while maintaining more moderate confidence for nuanced cases.</p> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Haotian Zhao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>