<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> How Terrifying the Dream Is：A Word Embedding Approach | Haotian Zhao </title> <meta name="author" content="Haotian Zhao"> <meta name="description" content="This project uses word embedding technology to quantitatively analyze and measure the emotional tone of dream descriptions, revealing the underlying sentiments of our subconscious narratives."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%A1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://haotianzhao99.github.io/projects/wordembedding/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Haotian</span> Zhao </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/academicpapers/">academic papers </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">How Terrifying the Dream Is：A Word Embedding Approach</h1> <p class="post-description">This project uses word embedding technology to quantitatively analyze and measure the emotional tone of dream descriptions, revealing the underlying sentiments of our subconscious narratives.</p> </header> <article> <h1 id="word-embedding">Word Embedding</h1> <p>Word embedding is a technique in natural language processing (NLP) and machine learning where words or phrases are represented as vectors of real numbers. These vectors capture semantic and syntactic relationships between words, allowing machines to process and understand language more effectively.</p> <p>Suppose we are calculating a text that contains a total of four words: cat, dog, cow, sheep. Each position in the vector represents a word.</p> <div class="row justify-content-center"> <div class="col-md-6 col-lg-4 mt-3 mt-md-0"> <figure class="figure"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project/wordembedding/word1.png" sizes="95vw"></source> <img src="/assets/img/project/wordembedding/word1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="word embedding" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="figure-caption text-center">When words are semantically similar, they are also close in space.</figcaption> </figure> </div> </div> <style>.figure{max-width:100%;height:auto}.figure img{max-width:100%;height:auto}</style> <p>A standard approach to measure the similarity between embedding vectors is to compute their cosine similarity. To measure the similarity between two embedding vectors using cosine similarity, the formula is:</p> \[\text{cosine similarity}(\vec{A}, \vec{B}) = \frac{\vec{A} \cdot \vec{B}}{\|\vec{A}\| \|\vec{B}\|}\] <p>where:</p> <ul> <li>\(\vec{A}\) and \(\vec{B}\) are the embedding vectors.</li> <li>\(\vec{A} \cdot \vec{B}\) is the dot product of vectors \(\vec{A}\) and \(\vec{B}\).</li> <li>\(\|\vec{A}\|\) and \(\|\vec{B}\|\) are the magnitudes (or norms) of vectors \(\vec{A}\) and \(\vec{B}\), respectively.</li> </ul> <h1 id="how-terrifying-the-dream-is-a-word-embedding-approach">How Terrifying the Dream Is: A Word Embedding Approach</h1> <p>Using a rich textual <a href="https://www.kaggle.com/datasets/sarikakv1221/dreams" rel="external nofollow noopener" target="_blank">dataset</a> comprising over 30,000 dream descriptions from Kaggle, we are able to train word embedding models. This diverse dataset encompasses a wide spectrum of dream narratives, ranging from mundane everyday occurrences to surreal fantasies, from spine-chilling nightmares to blissful reveries. Such variety enabled our models to capture the intricate nuances and multifaceted nature of dreams. The trained models are not only capable of identifying semantically similar words within dream texts but also further reveal the profound implications embedded within dreams.For instance, the features and characteristics of dreams: Is it a pleasant dream or a nightmare? To what extent is it a nightmare?</p> <h2 id="modelling-approach">Modelling approach</h2> <p>When delving into textual data describing dreams, we can employ word embedding models. Algorithms such as Word2vec or GloVe are utilized to transform all dream-related textual data into word vectors within a high-dimensional space. These word vectors capture the semantic and syntactic information of words, thereby laying the groundwork for subsequent analysis.</p> <p>In some studies, we have observed that through word embedding techniques, scholars are able to measure the degree of stigmatization of diseases <a href="https://journals.sagepub.com/doi/10.1177/00031224231197436" rel="external nofollow noopener" target="_blank">(Best &amp; Arseniev-Koehler, 2023)</a>, the level of novelty in texts <a href="https://journals.sagepub.com/doi/10.1177/00031224221123030" rel="external nofollow noopener" target="_blank">(Zhou, 2022)</a>, and the morality of the political rhetoric <a href="https://www.cambridge.org/core/journals/british-journal-of-political-science/article/lexical-ambiguity-in-political-rhetoric-why-morality-doesnt-fit-in-a-bag-of-words/BF369893D8B6B6FDF8292366157D84C1" rel="external nofollow noopener" target="_blank">(Kraft &amp; Klemmensen, 2024)</a>, among other aspects.</p> <p>Following their approaches, once the model training is complete, we can conduct a series of exploratory analyses to assess the dream descriptions. For instance, we can calculate the cosine similarity between word vectors to explore how common concepts in dreams are interrelated. Furthermore, by analyzing the average of word vectors, we can evaluate the emotional tendencies and themes of the entire dream narrative.</p> <h2 id="training-our-own-embedding">Training our Own Embedding</h2> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/wordembedding.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p>Through the implementation of word embedding techniques, we have successfully developed a preliminary methodology for quantifying the perceived terror level in dream narratives. However, it is crucial to acknowledge that the aforementioned code serves merely as a proof of concept and exhibits several limitations that warrant further refinement.</p> <p>Firstly, the constrained scope of our training corpus may impede the model’s ability to discern nuanced semantic variations or potentially introduce unintended biases. This limitation could be addressed by expanding and diversifying the training dataset to encompass a broader range of dream descriptions and emotional contexts.</p> <p>Secondly, the reliance on a single lexical item—”terrifying”—as the sole metric for terror assessment presents an oversimplified approach to a complex psychological phenomenon. Dreams often evoke a spectrum of emotions and sensations that may not be adequately captured by a unidimensional measure.</p> <p>To enhance the robustness and validity of our model, future iterations could rely on pre-trained word embeddings (GloVe), or incorporate pre-trained large language models like BERT, GPT, or Gemma through transfer learning and fine-tuning could significantly improve the model’s language understanding capabilities. Optimizing the model architecture by introducing attention mechanisms or adopting Transformer-based structures would allow for better capture of complex contextual relationships. Expanding and augmenting the training dataset, combined with advanced regularization methods and curriculum learning strategies, could further enhance the model’s generalization ability. Finally, incorporating domain-specific knowledge and targeted fine-tuning would enable the model to better adapt to specific task requirements. The combined application of these optimization measures would substantially improve the model’s performance across various complex language understanding tasks, making it more reliable and efficient.</p> <p>Furthermore, the incorporation of advanced natural language processing techniques, such as sentiment analysis and emotion detection algorithms, could provide a more nuanced understanding of the emotional landscape within dream narratives. This multifaceted approach would not only improve the accuracy of terror quantification but also offer insights into the broader emotional and cognitive aspects of dream content.</p> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Haotian Zhao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>